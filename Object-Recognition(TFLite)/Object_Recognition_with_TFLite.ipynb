{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition Using Tensorflow Lite with MobilenetV2 Prebuild Model\n",
    "**Full Script At [Object_Recognition_(TensorFlow_Lite_Edition).py](/Object-Recognition(TFLite)/Object_Recognition_(TensorFlow_Lite_Edition).py)**\n",
    "<br>\n",
    "*Libraries Used:* \n",
    "- tflite_runtime \n",
    "- OpenCV\n",
    "- numpy \n",
    "- Picamera2 \n",
    "- libcamera \n",
    "- os\n",
    "- time "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installing Libraries @ Dependencies \n",
    "- Build-In Libraries \n",
    "    - Picamera2 \n",
    "    - libcamera\n",
    "    - os\n",
    "    - time \n",
    "\n",
    "- User Installed Libraries\n",
    "    - tflite_runtime \n",
    "    - OpenCV \n",
    "    - numpy \n",
    "\n",
    "### First, let's install the user install libraries ! \n",
    "1. tflite_runtime \n",
    "    - Follow the link [Tensorflow.org](https://www.tensorflow.org/lite/guide/python)\n",
    "2. OpenCV \n",
    "    - Follow the link [OpenCV.org](https://docs.opencv.org/4.x/d2/de6/tutorial_py_setup_in_ubuntu.html)\n",
    "3. NumPy \n",
    "    - Follow the link [numpy.org](https://numpy.org/install/)\n",
    "\n",
    "\n",
    "### Verify the installed libraries \n",
    "1. tflite_runtime \n",
    "    - In Python File type: <br>\n",
    "    `import tflite_runtime.interpreter as tflite` \n",
    "    - Installation sucess if codeing run without error  \n",
    "2. OpenCV \n",
    "    - In Python File type: <br>\n",
    "    `import cv2` <br>\n",
    "    `print(cv2.__version__)`\n",
    "    - The coding above will print the version of opencv library installed  \n",
    "3. NumPy \n",
    "    - In Python File type: <br>\n",
    "    `import numpy as np`\n",
    "    - Installation sucess if codeing run without error "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading The model Files \n",
    "The Object Detection Model used in this script is: <br>\n",
    "***MobilenetV2(tflite)***<br>\n",
    "Download the object detection Model from github\n",
    "1. Open the terminal in Raspberry Pi \n",
    "2. Type the command bellow:\n",
    "- `git clone https://https://github.com/raspberrypi/picamera2` <br>\n",
    "This will download the Picamera2 repository from Github \n",
    "3. Nevigate to the Model folder:<br>\n",
    "picamera2 --> examples --> tensorflow\n",
    "4. Multiple Model is included in the folder \n",
    "5. In this script the Model used is ***mobilenet_v2.tflite***\n",
    "6. Beside the Model, The label is also needed.\n",
    "7. The labels used for this script is ***coco_lebels.txt***\n",
    "8. Copy the two target files above to the same directory of the script. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's Start Coding ! \n",
    "### 1. Import all the libraries \n",
    "- Tensorflow Lite \n",
    "- OpenCV \n",
    "- NumPy \n",
    "- Picamera2 \n",
    "- libcamera\n",
    "- os\n",
    "- time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorflow Lite Library\n",
    "import tflite_runtime.interpreter as tflite\n",
    "\n",
    "## For Image processing \n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "## To capture the frames from the camera\n",
    "from picamera2 import Picamera2\n",
    "from libcamera import controls\n",
    "\n",
    "## For validation of the model and label files \n",
    "import os\n",
    "\n",
    "## To get the accurate time \n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Verify and import the model file and label files\n",
    "- Check if the script is able to find the files using the os library \n",
    "- Exit the script if the model and label files are not found\n",
    "\n",
    "****In this script all the Model and lebel file are placed under a folder called *tebsorflow_lite_examples***\n",
    "<br>\n",
    "***Thus, the mode_folder variable is created.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the model and label files\n",
    "model_folder = 'tensorflow_lite_examples'\n",
    "model_file = '/mobilenet_v2.tflite'\n",
    "model_path = model_folder + model_file\n",
    "label_file = '/coco_labels.txt'\n",
    "label_path = model_folder + label_file\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Model file not found\")\n",
    "    print(\"Please check the path: \", model_path)\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Model file found at:\", model_path)\n",
    "\n",
    "if not os.path.exists(label_path):\n",
    "    print(\"Label file not found\")\n",
    "    print(\"Please check the path: \", label_path)\n",
    "    exit()\n",
    "else:\n",
    "    print(\"Label file found at:\", label_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read the label file and convert it to match the model class \n",
    "- The code bellow will open the coco_label.txt in read mode \n",
    "- It will read through all the lines and saperate the class ID and class name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(label_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    labels = {int(line.strip().split(maxsplit=1)[0]): line.strip().split(maxsplit=1)[1] for line in lines}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Initialise the Picamera2 library and start the camera \n",
    "- Initialise the Picamera2 Library and configure it to output the video format as:\n",
    "    - Color Format: \"XRGB8888\" \n",
    "    - Frame Size: \"640(Width), 480(Height)\"\n",
    "- The code `cam.set_controls({\"AfMode\": controls.AfModeEnum.Continuous})` will enable the Auto Focus function \n",
    "* The auto focus function only available on Raspberry Pi Camera 3 and above. Raspberry Pi Camera 1 and 2 does not support the Auto Focus Function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the camera\n",
    "frame_height = 480\n",
    "frame_width = 640\n",
    "cam = Picamera2()\n",
    "cam.configure(cam.create_preview_configuration(main={\"format\": 'XRGB8888', \"size\": (frame_width, frame_height)}))\n",
    "cam.start()\n",
    "cam.set_controls({\"AfMode\": controls.AfModeEnum.Continuous})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Define Object Detection function \n",
    "\n",
    "#### Initialise the Tensorflow Lite Library \n",
    "- Point the derectory of Model path to TensorFlow Lite <br>\n",
    "`interpreter = tflite.Interpreter(model_path=model_path)`\n",
    "- Initialise the tensorflow Library \n",
    "    - Set the *input_details*\n",
    "    - Set the *output_details*\n",
    "    - Set the *height* of object detection\n",
    "    - Set the *width* of object detection  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def object_detection():\n",
    "    interpreter = tflite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "    height = input_details[0]['shape'][1]\n",
    "    width = input_details[0]['shape'][2]\n",
    "    \n",
    "    \n",
    "    \n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Normalise and Insert the data To TensorFlow Lite for Detection \n",
    " \n",
    "- Obtained the captured frame from the camera and store to the variable *frame* <br>\n",
    "``` \n",
    "    frame = cam.capture_array()\n",
    "```\n",
    "- Obtain the time and store it in the t_start variable for FPS calculation<br>\n",
    "``` \n",
    "    t_start = time.time()\n",
    "```\n",
    "- Initalise the variable *fps* <br>\n",
    "``` \n",
    "    fps = 0\n",
    "```\n",
    "- Convert the color format of the captured from *RGB* to *BGR* using OpenCV<br>\n",
    "``` \n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)]\n",
    "``` \n",
    "- Resize the frame to fit the object detection algrothim and pass it to tensorflow lite of object dedtection<br>\n",
    "```\n",
    "    resized_frame = cv2.resize(rgb_frame, (width, height))\n",
    "    input_data = np.expand_dims(resized_frame, axis=0) \n",
    "```\n",
    "- Nomalize and Convert the data to ready for TensorFlow Lite Object Recognition \n",
    "``` \n",
    "    if input_details[0]['dtype'] == np.float32:\n",
    "        input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "``` \n",
    "- Insert the data to TensorFlow Lite and start the detection process \n",
    "```\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "        frame = cam.capture_array()\n",
    "        t_start = time.time()\n",
    "        fps = 0\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        resized_frame = cv2.resize(rgb_frame, (width, height))\n",
    "        input_data = np.expand_dims(resized_frame, axis=0)\n",
    "\n",
    "        if input_details[0]['dtype'] == np.float32:\n",
    "            input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "\n",
    "        interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "        interpreter.invoke()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Obtain the Detected data (Object) \n",
    "- Get the coordinates of the detected Object \n",
    "```\n",
    "    detected_boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "```\n",
    "- Get the Class ID of the detected Object \n",
    "```\n",
    "    detected_classes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "```\n",
    "- Get the Score of the detected object (Similarity)\n",
    "```\n",
    "    detected_scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "```\n",
    "- Get the number of object detected in a frame \n",
    "```\n",
    "    num_boxes = int(interpreter.get_tensor(output_details[3]['index'])[0])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        detected_boxes = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "        detected_classes = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "        detected_scores = interpreter.get_tensor(output_details[2]['index'])[0]\n",
    "        num_boxes = int(interpreter.get_tensor(output_details[3]['index'])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. For loop (For all Detected Object)\n",
    "- Only execute if the score of the detected object is more than 0.5 (50%)\n",
    "- Extract the coordinates from the detected_box variable \n",
    "```\n",
    "    ymin, xmin, ymax, xmax = detected_boxes[i]\n",
    "```\n",
    "- Get the hight and width of the detected object \n",
    "```\n",
    "    im_height, im_width, _ = frame.shape\n",
    "```\n",
    "- Calculate the Relative coordinates of the detected object \n",
    "```\n",
    "    left = int(xmin * im_width)\n",
    "    right = int(xmax * im_width)\n",
    "    top = int(ymin * im_height)\n",
    "    bottom = int(ymax * im_height)   \n",
    "    center_x = int((left + right) // 2)\n",
    "    center_y = int((top + bottom) // 2)\n",
    "```\n",
    "- From the calculated coordinates using OpenCV, Draw the a rectangle box \n",
    "```\n",
    "    cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)  \n",
    "```\n",
    "\n",
    "- Ensure the class Id is in the label list and find the name in the label file \n",
    "```\n",
    "    class_id = int(detected_classes[i])\n",
    "    if class_id in labels:\n",
    "        label = labels[class_id]\n",
    "    else:\n",
    "        label = 'Unknown' \n",
    "```\n",
    "- Display the name of the detected object \n",
    "```\n",
    "    print(f\"Detected class ID: {class_id}, Label: {label}, Score: {detected_scores[i]}\") \n",
    "    cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)  \n",
    "```\n",
    "- Calculate the FPS of the script and display the frame \n",
    "```\n",
    "    fps += 1\n",
    "    mfps = fps / (time.time() - t_start)\n",
    "    cv2.putText(frame, \"FPS : \" + str(int(mfps)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "    cv2.imshow(\"main\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break    \n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        for i in range(num_boxes):\n",
    "           if detected_scores[i] > 0.5:\n",
    "               ymin, xmin, ymax, xmax = detected_boxes[i]\n",
    "               im_height, im_width, _ = frame.shape\n",
    "               left = int(xmin * im_width)\n",
    "               right = int(xmax * im_width)\n",
    "               top = int(ymin * im_height)\n",
    "               bottom = int(ymax * im_height)   \n",
    "               center_x = int((left + right) // 2)\n",
    "               center_y = int((top + bottom) // 2)\n",
    "               print(\"Coordinates: \", \"\\nX: \", center_x, \"\\nY: \", center_y)\n",
    "               cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)   \n",
    "               # Ensure detected class index is within the range of labels\n",
    "               class_id = int(detected_classes[i])\n",
    "               if class_id in labels:\n",
    "                   label = labels[class_id]\n",
    "               else:\n",
    "                   label = 'Unknown'    \n",
    "               # Debugging output to verify label and class index\n",
    "               print(f\"Detected class ID: {class_id}, Label: {label}, Score: {detected_scores[i]}\") \n",
    "               cv2.putText(frame, label, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)   \n",
    "        fps += 1\n",
    "        mfps = fps / (time.time() - t_start)\n",
    "        cv2.putText(frame, \"FPS : \" + str(int(mfps)), (10, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        cv2.imshow(\"main\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Run Object Detection Funuction \n",
    "- When the script start run object detection function <br>\n",
    "`object_detection()` \n",
    "- When Keyboard Inturrupt (Ctrl + C)\n",
    "    - Stop the camera \n",
    "    - exit the script \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if __name__ == '__main__':\n",
    "        object_detection()\n",
    "except KeyboardInterrupt:\n",
    "    cam.close()\n",
    "    print(\"Exiting\")\n",
    "    exit()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
